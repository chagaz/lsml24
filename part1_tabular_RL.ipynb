{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs\n",
        "\n",
        "\n",
        "Note: this notebook is inspired from this github: https://github.com/rlgammazero/mvarl_hands_on, by Matteo Pirotta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlEnGsYOX5A"
      },
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# **[Exercice 1]** Understanding Value-Function and Q-function\n",
        "\n",
        "In this exercice, we are going to learn:\n",
        "\n",
        "*   What is a MDP?\n",
        "*   How to evaluate the quality of a policy in a MDP (Value-iteration and Policy-Iteration)\n",
        "*   How to move from V-function to Q-function\n",
        "*   How to move from Q-function to greedy-policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "## **[Step 1]** Dealing with MDP and RL environment\n",
        "\n",
        "Here, we are going to use the cleaning robot MDP from\n",
        "http://www.incompleteideas.net/sutton/book/first/3/node7.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DVrtCu1xJ6",
        "cellView": "form"
      },
      "source": [
        "# @title **[Skip]** Robot MDP implementation\n",
        "\n",
        "class RobotEnv:\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self._RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        self._Ns = 2\n",
        "        self._Na = 3\n",
        "        self._gamma = gamma\n",
        "\n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        self._P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self._state_decoder  = {0: \"High\", 1: \"Low\"}\n",
        "        self._action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "\n",
        "        # Initialize base class\n",
        "        self._states = np.arange(self.Ns).tolist()\n",
        "        self._action_sets = [np.arange(self.Na).tolist()]*self.Ns\n",
        "\n",
        "    ### Utils\n",
        "    def render_state(self, state):\n",
        "      return self._state_decoder[state]\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self._action_decoder[action]\n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self._state_decoder[i], self._action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return self._states\n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return self._action_sets\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self._P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self._R\n",
        "\n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self._Ns\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return self._Na\n",
        "\n",
        "    ### Interact with environment\n",
        "    def reward_func(self, state, action, *_):\n",
        "      return self._R[state, action]\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self._P[s,a,:]\n",
        "        next_s = self._RS.choice(self.states, p = prob)\n",
        "        return next_s\n",
        "\n",
        "    def reset(self, new_initial_state=0):\n",
        "        assert new_initial_state < self.Ns\n",
        "        self.state = new_initial_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self._state_decoder[state],\n",
        "            self._action_decoder[action],\n",
        "            self._state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqt4muO1yij"
      },
      "source": [
        "# create the environment\n",
        "env = RobotEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbC3m3wO9Imp"
      },
      "source": [
        "A MDP have is a tuple with ($S$, $A$, $R$, $P$, $\\gamma$)\n",
        "*   $S$ is the state space\n",
        "*   $A$ is the action space\n",
        "*   $R$ is the reward function\n",
        "*   $P$ is the transition kernel. If I am in state $s$, and take the action $a$, what is the probability of moving to state $s'$\n",
        "*   $\\gamma$ is the discount factor, i.e., how far in the future you are looking for rewards (gamma=0 means, you just take immediate reward, gammma=0.9 you look at reward around 10 steps away)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FA5vUBd7X9q"
      },
      "source": [
        "# Display some of the MDP relevant information\n",
        "\n",
        "print(\"Number of states: \", env.Ns, [env.render_state(s) for s in range(env.Ns)])\n",
        "print(\"Number of actions: \", env.Na, [env.render_action(a) for a in range(env.Na)])\n",
        "print(\"\")\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of available actions per state:\", env.actions)\n",
        "print(\"\")\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_hP0wjEFT4"
      },
      "source": [
        "A MDP is a mathematical representation of an environment. Here, we are going to interact with this environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6S_ja2FDXHP"
      },
      "source": [
        "state=1\n",
        "action=1\n",
        "print(f\"State {state}: battery is\", env.render_state(state))\n",
        "print(f\"Action {action}: robot performs\", env.render_action(action))\n",
        "print(f\"Reward at state={state} and action={action}) is\", env.reward_func(state,action))\n",
        "\n",
        "next_state = env.sample_transition(state,action)\n",
        "print(\"Next (stochastic) state is\", env.render_state(next_state))  # you can keep running this cell colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TPqLe-LP_"
      },
      "source": [
        "Finally, we here define a helper to step in the environment. Let's try to follow a random policy by picking a random action $a$ at everytime step $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OC-zSnd9ExV"
      },
      "source": [
        "# Interact with environment\n",
        "\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(15):\n",
        "    action = np.random.randint(env.Na) # Pick random action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\")\n",
        "    if done:\n",
        "        print('You ran out of battery')\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z7SImS4Fpta"
      },
      "source": [
        "It is also possible to define a deterministic policy which associate an action $a$ for every state $s$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGmYI3OCKXv"
      },
      "source": [
        "# A random deterministic policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1883coHXIF"
      },
      "source": [
        "### **[Question 1]** Handcrafting the optimal policy\n",
        "Hand-craft the optimal policy (High=search, Low=recharge), display it, and interact with the environment for 5 steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApI4TrT-HWkC"
      },
      "source": [
        "my_policy = ...\n",
        "\n",
        "# Interaction loop\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(5):\n",
        "    action = my_policy[state] # Pick action according to the policy\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\")\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMk_C05rGGVs"
      },
      "source": [
        "From now on, you should have understood how to interact with an environment, and retrieve the MDP information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7d9_NsHQXM"
      },
      "source": [
        "## **[Step 2]** Evaluating a policy\n",
        "In this subsection, we aim at estimating the quality of a predefined policy, i.e, how much reward can I expect if I follow any policy (even if this policy is not optimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "\n",
        "### Useful functions\n",
        "In the following exercice, there is a constant back-and-forth between dense and sparse representation of policy. For instance, taking the action $a=2$ may be encoded by:\n",
        "\n",
        "*   Sparse Represention: a=2\n",
        "*   Dense Represention: a=[0, 0, 1]\n",
        "\n",
        "To help you to move from dense and sparse, policy, we provide you those two functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sekFIFmm6V71"
      },
      "source": [
        "def densify_policy(policy, Na):\n",
        "  \"\"\" Turn a sparse policy into a dense one.\n",
        "  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  \"\"\"\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  \"\"\" Turn a dense determinist policy into a sparse one.\n",
        "  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  \"\"\"\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT3iHZ8JNLO"
      },
      "source": [
        "### **[Question 2]** Policy Evaluation\n",
        "Let's start doing things with our policy!\n",
        "\n",
        "Compute the dynamics and rewards given the policy, and solve the linear system on V to evaluate the policy.\n",
        "\n",
        "First, compute the policy normalized transition/rewards\n",
        "$$P^{\\pi}(s, s') = \\sum_a{\\pi(s|a)P(s,a,s')}$$\n",
        "$$R^{\\pi}(s) = \\sum_a{\\pi(s|a)R(s,a)}$$\n",
        "\n",
        "Then, compute the value function, by solving Bellman equation,\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "i.e.\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "$$V^{\\pi} - \\gamma P^{\\pi}V^{\\pi} = R^{\\pi}$$\n",
        "$$(I - \\gamma P^{\\pi})V^{\\pi} = R^{\\pi}$$\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1}R^{\\pi}$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-CumBrBy-0cR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$(I - \\gamma P^{\\pi})V^{\\pi} = R^{\\pi}$$\n",
        "$$A X = B$$ with $A = (I - \\gamma P^{\\pi})$, $X = V^\\pi$ and $B = R^\\pi$\n",
        "What I want is to compute $X = A^{-1}B$\n"
      ],
      "metadata": {
        "id": "FDMWiskJIJr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy evaluation (exact)\n",
        "from typing import *\n",
        "\n",
        "def build_Ppi_Rpi(my_env, sparse_policy) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "  # Retrieve the environment MDP\n",
        "  P = my_env.transition_matrix\n",
        "  R = my_env.reward_matrix\n",
        "  gamma = my_env.gamma\n",
        "\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute the dynamics given the policy\n",
        "  Ppi = ...\n",
        "  Rpi = ...\n",
        "\n",
        "  return Ppi, Rpi\n",
        "\n",
        "def build_Vpi(Ppi: np.ndarray,\n",
        "            Rpi: np.ndarray,\n",
        "            ) -> np.ndarray:\n",
        "  # Evaluate the policy\n",
        "  Vpi = ...\n",
        "\n",
        "  return Vpi"
      ],
      "metadata": {
        "id": "cLW68SEjxeqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code !\n",
        "sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "print(\"Ppi\", Ppi)\n",
        "print(\"Rpi\", Rpi)\n",
        "\n",
        "Vpi = build_Vpi(Ppi, Rpi)\n",
        "\n",
        "print(\"Vpi\", Vpi)"
      ],
      "metadata": {
        "id": "Mj1QTUf5bqda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnODmP6TR1-_"
      },
      "source": [
        "### **[Question 2]** Implement the recursive implementation of value evaluation\n",
        "\n",
        "You want to evaluate the policy by iterating the fixed point equation on V, starting from a randomly initialized V function.\n",
        "\n",
        "In other words:\n",
        "\n",
        "To compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "you can use the contractive property of Bellman\n",
        "$$V^{\\pi}_{k+1} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}_k$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHiqwbKSCCc"
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Stopping criterion\n",
        "# Feel free to use the any valid stopping criterion (max_iteration or inf_norm)\n",
        "epsilon = 1e-7\n",
        "\n",
        "# Retrieving Ppr and Rpi using the function you built\n",
        "Ppi, Rpi = build_Ppi_Rpi(env, sparse_policy)\n",
        "\n",
        "def build_Vpi_iterative(Ppi: np.ndarray,\n",
        "                        Rpi: np.ndarray,\n",
        "                        epsilon: float,\n",
        "                        ) -> np.ndarray:\n",
        "\n",
        "  # Estimate V (please print v at each iteration k)\n",
        "  v = np.zeros((Ppi.shape[0],))\n",
        "  next_v = None\n",
        "  while ...:\n",
        "    if next_v is not None:\n",
        "      v = next_v\n",
        "    next_v = ...\n",
        "\n",
        "  return next_v\n",
        "\n",
        "Vpi = build_Vpi_iterative(Ppi, Rpi, epsilon)\n",
        "print(build_Vpi_iterative(Ppi, Rpi, epsilon))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcvnxbuOa1z"
      },
      "source": [
        "### **[Question 3]** Turning V-function into Q-function\n",
        "What is the Q-function for this value function ?\n",
        "\n",
        "$$Q^{\\pi}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s')V^{\\pi}(s')$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rLK8scKO0CV"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi = ...\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzH417WO7Nu"
      },
      "source": [
        "The Q-function is a useful way to evaluate the policy. Yet, it can also be used to improve the policy! To do so, you can create a new policy by taking the argmax of the Q-function (improvment step).\n",
        "\n",
        "What is the next policy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvglCCIjPUaf"
      },
      "source": [
        "# Compute the Q values\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "new_policy = ...\n",
        "print(\"## new pi:\")\n",
        "print(new_policy)\n",
        "print(env.render_policy(new_policy))\n",
        "\n",
        "# Compute the value of the NEW policy\n",
        "new_dense_policy = densify_policy(new_policy, env.Na)\n",
        "\n",
        "new_Ppi, new_Rpi = build_Ppi_Rpi(env, new_policy)\n",
        "new_Vpi = build_Vpi(new_Ppi, new_Rpi)\n",
        "print(new_Vpi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0Iu7xMP9Wp"
      },
      "source": [
        "## **[Step 3]** Policy Improvement in MDP\n",
        "\n",
        "In slide 50-52, we introduced two algorithms to obtain the optimal policy from a sub-optimal one (by using different shade of policy improvment shapes).\n",
        "\n",
        "*   **Policy iteration**: From an initial policy, compute its value exactly, then perform one step of greedy policy improvement.\n",
        "*   **Value iteration**: From an initial policy, compute its value approximately, then perform one step of greedy policy improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNY2iNLpVYCG"
      },
      "source": [
        "# @title **[Skip]** New Environment\n",
        "\n",
        "\n",
        "class GridWorldWithPits:\n",
        "    def __init__(self, grid, txt_map, gamma=0.99, proba_succ=0.95, uniform_trans_proba=0.001, normalize_reward=False):\n",
        "        self.desc = np.asarray(txt_map, dtype='c')\n",
        "        self.grid = grid\n",
        "        self.txt_map = txt_map\n",
        "\n",
        "        self.action_names = np.array(['right', 'down', 'left', 'up'])\n",
        "\n",
        "        self.n_rows, self.n_cols = len(self.grid), max(map(len, self.grid))\n",
        "\n",
        "        # Create a map to translate coordinates [r,c] to scalar index\n",
        "        # (i.e., state) and vice-versa\n",
        "        self.normalize_reward = normalize_reward\n",
        "\n",
        "\n",
        "        self.initial_state = None\n",
        "        self.coord2state = np.empty_like(self.grid, dtype=int)\n",
        "        self.nb_states = 0\n",
        "        self.state2coord = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(len(self.grid[i])):\n",
        "                if self.grid[i][j] != 'w':\n",
        "                    if self.grid[i][j] == 's':\n",
        "                        self.initial_state = self.nb_states\n",
        "                    self.coord2state[i, j] = self.nb_states\n",
        "                    self.nb_states += 1\n",
        "                    self.state2coord.append([i, j])\n",
        "                else:\n",
        "                    self.coord2state[i, j] = -1\n",
        "\n",
        "        self.P = None\n",
        "        self.R = None\n",
        "        self.proba_succ = proba_succ\n",
        "        self.uniform_trans_proba = uniform_trans_proba\n",
        "\n",
        "        # compute the actions available in each state\n",
        "        self.state_actions = [range(len(self.action_names)) for _ in range(self.nb_states)]#self.compute_available_actions()\n",
        "        self.matrix_representation()\n",
        "        self.lastaction = None\n",
        "        self.current_step = 0\n",
        "\n",
        "        self._actions = self.state_actions\n",
        "        self._gamma = gamma\n",
        "\n",
        "\n",
        "    def matrix_representation(self):\n",
        "        if self.P is None:\n",
        "            nstates = self.nb_states\n",
        "            nactions = max(map(len, self.state_actions))\n",
        "            self.P = np.inf * np.ones((nstates, nactions, nstates))\n",
        "            self.R = np.inf * np.ones((nstates, nactions))\n",
        "            for s in range(nstates):\n",
        "                r, c = self.state2coord[s]\n",
        "                for a_idx, action in enumerate(range(len(self.action_names))):\n",
        "                    self.P[s, a_idx].fill(0.)\n",
        "                    if self.grid[r][c] == 'g':\n",
        "                        self.P[s, a_idx, self.initial_state] = 1.\n",
        "                        self.R[s, a_idx] = 10.\n",
        "                    else:\n",
        "                        ns_succ, ns_fail = np.inf, np.inf\n",
        "                        if action == 0:\n",
        "                            ns_succ = self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ns_fail = [self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[max(0, r - 1), c]\n",
        "                            ]\n",
        "\n",
        "                        elif action == 1:\n",
        "                            ns_succ = self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ns_fail = [self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ]\n",
        "                        elif action == 2:\n",
        "                            ns_succ = self.coord2state[r, max(0, c - 1)]\n",
        "                            ns_fail = [self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ]\n",
        "                        elif action == 3:\n",
        "                            ns_succ = self.coord2state[max(0, r - 1), c]\n",
        "                            ns_fail = [self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[r, max(0, c - 1)]\n",
        "                            ]\n",
        "\n",
        "                        L = []\n",
        "                        for el in ns_fail:\n",
        "                            x, y = self.state2coord[el]\n",
        "                            if self.grid[x][y] == 'w':\n",
        "                                L.append(s)\n",
        "                            else:\n",
        "                                L.append(el)\n",
        "\n",
        "                        self.P[s, a_idx, ns_succ] = self.proba_succ\n",
        "                        for el in L:\n",
        "                            self.P[s, a_idx, el] += (1. - self.proba_succ)/len(ns_fail)\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] + self.uniform_trans_proba / nstates\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] / np.sum(self.P[s, a_idx])\n",
        "\n",
        "                        assert np.isclose(self.P[s, a_idx].sum(), 1)\n",
        "\n",
        "                        if self.grid[r][c] == 'x':\n",
        "                            self.R[s, a_idx] = -20\n",
        "                        else:\n",
        "                            self.R[s, a_idx] = -2\n",
        "\n",
        "            if self.normalize_reward:\n",
        "                minr = np.min(self.R)\n",
        "                maxr = np.max(self.R[np.isfinite(self.R)])\n",
        "                self.R = (self.R - minr) / (maxr - minr)\n",
        "\n",
        "            self.d0 = np.zeros((nstates,))\n",
        "            self.d0[self.initial_state] = 1.\n",
        "\n",
        "    def compute_available_actions(self):\n",
        "        # define available actions in each state\n",
        "        # actions are indexed by: 0=right, 1=down, 2=left, 3=up\n",
        "        state_actions = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "                if self.grid[i][j] == 'g':\n",
        "                    state_actions.append([0])\n",
        "                elif self.grid[i][j] != 'w':\n",
        "                    actions = [0, 1, 2, 3]\n",
        "                    if i == 0:\n",
        "                        actions.remove(3)\n",
        "                    if j == self.n_cols - 1:\n",
        "                        actions.remove(0)\n",
        "                    if i == self.n_rows - 1:\n",
        "                        actions.remove(1)\n",
        "                    if j == 0:\n",
        "                        actions.remove(2)\n",
        "\n",
        "                    for a in copy.copy(actions):\n",
        "                        r, c = i, j\n",
        "                        if a == 0:\n",
        "                            c = min(self.n_cols - 1, c + 1)\n",
        "                        elif a == 1:\n",
        "                            r = min(self.n_rows - 1, r + 1)\n",
        "                        elif a == 2:\n",
        "                            c = max(0, c - 1)\n",
        "                        else:\n",
        "                            r = max(0, r - 1)\n",
        "                        if self.grid[r][c] == 'w':\n",
        "                            actions.remove(a)\n",
        "\n",
        "                    state_actions.append(actions)\n",
        "        return state_actions\n",
        "\n",
        "    def description(self):\n",
        "        desc = {\n",
        "            'name': type(self).__name__\n",
        "        }\n",
        "        return desc\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return self.R[state, action]\n",
        "\n",
        "    def reset(self, s=None):\n",
        "        self.lastaction = None\n",
        "        if s is None:\n",
        "            self.state = self.initial_state\n",
        "        else:\n",
        "            self.state = s\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action_index = self.state_actions[self.state].index(action)\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "\n",
        "        p = self.P[self.state, action_index]\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "        reward = self.R[self.state, action_index]\n",
        "\n",
        "        self.lastaction = action\n",
        "\n",
        "        r, c = self.state2coord[self.state]\n",
        "        done = self.grid[r][c] == 'g'\n",
        "        self.current_step +=1\n",
        "        self.state = next_state\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render_state(self, state):\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[state]\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if self.grid[r][c] == 'x':\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(out[1 + r][2 * c + 1], 'red', highlight=True)\n",
        "        elif self.grid[r][c] == 'g':  # passenger in taxi\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'green', highlight=True)\n",
        "        else:\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'yellow', highlight=True)\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self.action_names[action]\n",
        "\n",
        "    def render_policy(self, pol):\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[self.state]\n",
        "\n",
        "        for s in range(self.Ns):\n",
        "            r, c = self.state2coord[s]\n",
        "            action = pol[s]\n",
        "            # 'right', 'down', 'left', 'up'\n",
        "            if action == 0:\n",
        "                out[1 + r][2 * c + 1] = '>'\n",
        "            elif action == 1:\n",
        "                out[1 + r][2 * c + 1] = 'v'\n",
        "            elif action == 2:\n",
        "                out[1 + r][2 * c + 1] = '<'\n",
        "            elif action == 3:\n",
        "                out[1 + r][2 * c + 1] = '^'\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = GridWorldWithPits(grid=self.grid, txt_map=self.txt_map,\n",
        "                                    proba_succ=self.proba_succ, uniform_trans_proba=self.uniform_trans_proba)\n",
        "        return new_env\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        try:\n",
        "            p = self.P[s, a]\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return np.zeros([self.n_cols, self.n_rows])\n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return range(4)\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self.P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self.R\n",
        "\n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self.n_cols * self.n_rows\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbNd2WHi4Kb"
      },
      "source": [
        "The environment works as follow:\n",
        "\n",
        "*   At each timestep, you have a negative reward of -2\n",
        "*   If the agent moves to state labelled with X, it receives a negative reward of -20\n",
        "*   If the agent moves to the state labelled with G (goal), it receives a reward of 10, and the trajectory ends\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbuuOZj7VOgs"
      },
      "source": [
        "#@title New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2pOuPuFajI"
      },
      "source": [
        "#@title Relevant information about the environment\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhH4UyoFVp1"
      },
      "source": [
        "#@title Definining and running a random policy\n",
        "# Define a fixed random policy\n",
        "sparse_policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(5):\n",
        "    action = sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwIpq7j5SqwU"
      },
      "source": [
        "#@title Utility functions\n",
        "# useful function\n",
        "def plot_infnorm(all_v, v_star, name):\n",
        "  \"\"\"Print the infinite norm between computed vs and v_star (to get learning progress).\"\"\"\n",
        "\n",
        "  all_v = np.array(all_v)\n",
        "  v_star = np.array(v_star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(all_v - v_star).max(axis=1)\n",
        "\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(name)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6VKYj2xUWx"
      },
      "source": [
        "### **[Question 4]** Implement Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpp55q8QquM"
      },
      "source": [
        "# Compute Policy Iteration\n",
        "\n",
        "env.reset()\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "# Prepare v, and storage\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "\n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Perform your step of policy iteration here\n",
        "  Ppi = ...\n",
        "  Rpi = ...\n",
        "  Vpi = ...\n",
        "  Qpi = ...\n",
        "  new_sparse_policy = ...\n",
        "\n",
        "  # store v\n",
        "  v_all.append(Vpi)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion\n",
        "  if ...:\n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfgZcQneIUcH"
      },
      "source": [
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(10):\n",
        "    action = new_sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "# **[Exercice 2]** Q learning\n",
        "Q learning is a **model-free** algorithm for estimating the optimal Q-function **online**.\n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **off-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is (potentially) not the **learnt** one, and not the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment.\n",
        "- **Iterate**:\n",
        "  -  Pick an action according to an $\\varepsilon$-greedy version of the argmax policy on $Q$, i.e. with probability $\\varepsilon$, pick a random uniform action, with probability $1 - \\varepsilon$, pick the argmax of $Q(s, a)$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Update $Q$ using the quadruplet $(s, a, r, s')$ with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a'} Q(s', a'))$$\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmojgubLVL9"
      },
      "source": [
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 50000  # Increasee this number when it works for full convergence\n",
        "all_rewards = []\n",
        "all_q_function = []\n",
        "\n",
        "# Training values\n",
        "epsilon = 0.05\n",
        "alpha = 0.1\n",
        "\n",
        "# Initialize the Q-Function here\n",
        "q_function = ...\n",
        "\n",
        "while t < max_steps:\n",
        "\n",
        "    # Sample the action (epsilon greedy)\n",
        "    action = ...\n",
        "\n",
        "    # Sample the environment\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Update q-function\n",
        "    q_function[...] = ...\n",
        "\n",
        "    # Store information\n",
        "    all_rewards.append(reward)\n",
        "    all_q_function.append(np.copy(q_function))\n",
        "\n",
        "    state = next_state\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "\n",
        "    # iterate\n",
        "    t = t + 1\n",
        "\n",
        "sparse_policy = q_function.argmax(axis=1)\n",
        "print(env.render_policy(sparse_policy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZXjcsJMaoqK"
      },
      "source": [
        "#@title Plotting evolution of the reward across time\n",
        "average_rewards = np.convolve(all_rewards, np.ones(150)/150, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('rewards')\n",
        "# Now you have to implement the cumulative rewards :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwUdN1Xravru",
        "cellView": "form"
      },
      "source": [
        "#@title Plotting evolution of the q value across time\n",
        "state=1 #@param\n",
        "action=1 #@param\n",
        "one_q = [q[state, action] for q in all_q_function]\n",
        "average_q_values = np.convolve(one_q, np.ones(100)/100, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_q_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')\n",
        "# Now you have to implement the Q error :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VJo-11ubCN"
      },
      "source": [
        "#@title Running the agent:\n",
        "state = env.reset()\n",
        "env.render_state(state)\n",
        "rewards = []\n",
        "for i in range(30):\n",
        "    action = sparse_policy[state]\n",
        "    print(env.render_action(action))\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    print(env.render_state(state))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}