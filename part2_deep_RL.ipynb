{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **[Part 2]** Deep Reinforcement Learning\n",
        "\n",
        "In this notebook we make use of deep Reinforcement Learning, which can be seen as the encounter of Reinforcement Learning (environment, policy, Q-function...) and Deep Learning (neural networks, loss, gradients...)\n",
        "\n",
        "\n",
        "*Acknowledgement: code is inspired from PyTorch's documentation"
      ],
      "metadata": {
        "id": "2-57h2atyDqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Exercice 1]** REINFORCE"
      ],
      "metadata": {
        "id": "tzwXFAE0vlZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercice we are going to implement the REINFORCE algorithm, which is the \"simplest\" **on-policy** algorithm.\n",
        "\n",
        "\n",
        "We implement the policy $\\pi$ using a deep neural network of parameters $θ$, and we denote it by $\\pi_\\theta$:\n",
        "\n",
        "Recall the policy gradient formula (log-trick):\n",
        "\n",
        "Denoting $\\tau = (s_0,a_0, r_0, s_1, a_1 ,r_1, ..., s_{T-1}, a_{T-1},r_{T-1}, s_T)$ a trajectory in the MDP, the objective we aim to optimize is (policy performance):\n",
        "\n",
        "$$J(θ) = \\mathbb{E}_{\\tau \\sim \\pi_θ}[r(\\tau)] = ∫_\\tau r(\\tau)p_θ(\\tau)d\\tau$$\n",
        "\n",
        "Where $r(\\tau)= \\sum_{t\\geq 0} \\gamma^t r_t$ is the discounted reward associated to the trajectory, and $p_θ(\\tau)$ is the probability of the trajectory $\\tau$ under the policy $\\pi_\\theta$, it can be expressed as follows:\n",
        "\n",
        "$$ p_θ(\\tau) = ℙ(s_0) \\times \\big[ \\pi_θ(a_0|s_0) ℙ(r_0 |s_0, a_0)ℙ(s_1|s_0,a_0) \\big] \\times \\big[\\pi_θ(a_1|s_1) ℙ(r_1 | s_1, a_1)ℙ(s_2|s_1,a_1)\\big] \\times …$$\n",
        "\n",
        "$$p_θ(\\tau) = p(s_0) \\times \\prod_{t= 0}^{T-1} \\pi_θ(a_t|s_t)\\mathbb{P}(r_t|s_t,a_t)ℙ(s_{t+1}|s_t,a_t) $$\n",
        "\n",
        "Now, what interests us is the gradient of $J$ with respects to $θ$, because if we know how to compute it, we can optimize J with gradient descent, and get (hope to get) the best policy possible.\n",
        "\n",
        "To compute this gradient we make use of the log-trick, which is a classical technique to compute gradients of expectactions:\n",
        "\n",
        "\\begin{align*}\n",
        "∇_θJ(θ) &= ∇_θ \\Big[ ∫_\\tau r(\\tau)p_θ(\\tau)d\\tau \\Big] \\\\\n",
        "&= \\int_\\tau r(\\tau) ∇_θp_θ(\\tau) d\\tau \\\\\n",
        "&= \\int_\\tau r(\\tau) ∇_θp_θ(\\tau) \\times \\frac{p_θ(\\tau)}{p_θ(\\tau)} d\\tau \\\\\n",
        "&= \\int_\\tau r(\\tau)  \\times \\frac{∇_θp_θ(\\tau)}{p_θ(\\tau)} \\times p_θ(\\tau) d\\tau\\\\\n",
        "&= \\int_\\tau r(\\tau) ∇_\\theta\\big[\\log(p_\\theta)\\big](\\tau)p_θ(\\tau) d\\tau\\\\\n",
        "&= \\mathbb{E}_{\\tau \\sim \\pi_θ}\\Big[r(\\tau) ∇_\\theta\\big[\\log(p_\\theta)\\big](\\tau)\\Big]\n",
        "\\end{align*}\n",
        "\n",
        "Why are we quite happy with this ? $p_\\theta$ is a product, thus taking it's logarithm transforms it into a sum, and sums are quite easy to differentiate:\n",
        "\n",
        "$$\\log(p_\\theta(\\tau)) = \\log(p(s_0)) + \\sum_{t=0}^{T-1} \\Big[\\log (\\pi_\\theta(a_t|s_t)) + \\log(\\mathbb{P}(r_t|s_t,a_t)) + \\log(ℙ(s_{t+1}|s_t,a_t))  \\Big]$$\n",
        "\n",
        "And when we apply the gradient with respect to θ, most terms disappear and there only remains:\n",
        "\n",
        "$$ ∇_\\theta\\big[\\log(p_\\theta)\\big](\\tau) = \\nabla_θ \\log (\\pi_θ(a_t | s_t))$$\n",
        "\n",
        "And we obtain the **Policy Gradient** formula:\n",
        "\n",
        "$$\\nabla_θ J(θ) = \\mathbb{E}_{\\tau \\sim \\pi_θ}\\Big[\\sum_{t=0}^{T-1} \\nabla_θ \\log (\\pi_θ(a_t | s_t)) r(\\tau)\\Big] $$\n",
        "\n",
        "Now to get to the REINFORCE formula we will need another trick, I am going to skip the details but you can find them online (e.g. [here](https://web.stanford.edu/class/cs234/slides/lecture7post.pdf)), the idea is to use the temporal structure, and to remark that:\n",
        "\n",
        "$$\\mathbb{E}_{\\tau \\sim \\pi_θ}\\Big[ \\nabla_θ \\log (\\pi_θ(a_t | s_t)) r(\\tau)\\Big] = \\mathbb{E}_{\\tau \\sim \\pi_θ}\\Big[ \\nabla_θ \\log (\\pi_θ(a_t | s_t))\\sum_{t' =t}^{T-1}\\gamma^{t'}r_{t'}\\Big]$$\n",
        "\n",
        "And obtain the **REINFORCE** formula:\n",
        "\n",
        "$$ \\nabla_θ J(θ) = \\mathbb{E}_{\\tau \\sim \\pi_θ}\\Big[\\sum_{t=0}^{T-1} \\nabla_θ \\log (\\pi_θ(a_t | s_t)) \\sum_{t' =t}^{T-1}\\gamma^{t'}r_{t'}\\Big]$$\n",
        "\n",
        "\n",
        "\n",
        "This expectation is hard to compute, but what we can do is estimate it with Monte-Carlo, we generate $N$ trajectories by letting $\\pi_θ$ interact with the environment (that is why it is called a on-policy method) and compute the mean:\n",
        "\n",
        "\n",
        "\n",
        "$$ \\nabla_θ J(θ) ≈ \\frac{1}{N}\\sum_{i=1}^N\\Big[\\sum_{t=0}^{T-1} \\nabla_θ \\log (\\pi_θ(a_t^{(i)} | s_t^{(i)})) \\sum_{t' =t}^{T-1}\\gamma^{t'}r_{t'}^{(i)}\\Big]$$\n",
        "\n",
        "\n",
        "That we prefer to write (easier to implement):\n",
        "\n",
        "$$ \\nabla_θ J(θ) ≈ \\frac{1}{N}\\sum_{i=1}^N\\Big[\\sum_{t=0}^{T-1} \\gamma^t\\nabla_θ \\log (\\pi_θ(a_t^{(i)} | s_t^{(i)})) \\sum_{t' =t}^{T-1}\\gamma^{t'-t}r_{t'}^{(i)}\\Big]$$\n",
        "\n",
        "\n",
        "\n",
        "And we can now optimize $J$ with gradient descent using our favorite deep learning library.\n",
        "\n",
        "Note: we will work with $N=1$ in this practical, so it is the case of stochastic gradient descent.\n",
        "\n",
        "\n",
        "For this TP we are going to use:\n",
        "\n",
        "- **gym** for the environment part, gym is a popular RL framework introduced by OpenAI, that implements loads of environments.\n",
        "\n",
        "- **pytorch** for the deep learning part, pytorch is the most popular deep learning library, and is praised for its modulability and ease of use.\n"
      ],
      "metadata": {
        "id": "fWpDeA7K01g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributions as distributions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15, 5) # to have nice looking figs\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Check for GPU\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print(\"No GPU was detected, if you are on Colab, select T4 runtime and reexecute this cell\")\n",
        "  device = torch.device('cpu')\n",
        "else:\n",
        "  device = torch.device('cuda:0')\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "8A_j75jS1AcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kyzi5KEkvk3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment**: Cartpole\n",
        "\n",
        "We are going to work with the Cartpole environment, where we learn to balance a pole on a cart\n",
        "\n",
        "All info on the environment can be found on [gym's documentation](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
        "\n",
        "\n",
        "There are two actions in this MDP, push the pole to the left ot to the right, we model it by $\\mathcal{A}=\\{0, 1\\}$.\n",
        "\n",
        "The state space $\\mathcal{S}$ is 4-dimensionnal and continuous.\n",
        "\n",
        "Our policy will be a neural network that takes as input a vector $s\\in ℝ^4$ representing the state and outputs a vector $\\pi_\\theta(s)\\in [0,1]^2$ such that:\n",
        "\n",
        "$$ \\pi_θ(s) = \\begin{pmatrix} \\pi_θ(\\mathrm{left}|s) \\\\ \\pi_θ(\\mathrm{right}|s) \\end{pmatrix} $$\n",
        "\n",
        "The episode ends if the pole falls, or if the cart leaves the screen (gone too far on the sides), and we get a reward of 1 for each step taken.\n",
        "\n"
      ],
      "metadata": {
        "id": "GWBvHh8u_CKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment\n",
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "izyu1KfE_j69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The framework of gym is to use the step function:\n",
        "\n",
        "```\n",
        "state = env.reset() # Sample initial state\n",
        "\n",
        "for t in range(T):\n",
        "  action = policy(state) # Call policy to sample action\n",
        "  next_state, reward, terminated, truncated = env.step(action) # Act in the environment\n",
        "  state = next_state\n",
        "```\n",
        "\n",
        "Terminated and truncated are here to indicate if the trajectory ends (for example pole falls)\n",
        "\n"
      ],
      "metadata": {
        "id": "eg41vGOwYx6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Policy**: Multi-Layer Perceptron\n",
        "\n",
        "For this simple task we implement the policy using a MLP, in other deep RL contexts more complex architectures might be needed (notably CNNs to work with image inputs)\n",
        "\n",
        "\n",
        "**Question 1.** Implement a MLP with one hidden layer using torch\n",
        "\n",
        "It should use ReLU as as activation function, and have a dropout regularizer after the first layer.\n",
        "\n",
        "Finally the last layer should be a SoftMax in order to get probabilities for each action."
      ],
      "metadata": {
        "id": "cUzkBqxyDCZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # Apply all the layers to x\n",
        "      action_scores = ...  # Action scores is the output of the last layer\n",
        "      return F.softmax(action_scores, dim=1) # We pass it through a Softmax to obtain probabilities\n",
        "\n",
        "\n",
        "\n",
        "# Hint: if you never used pytorch before, the forward function usually looks like this\n",
        "\n",
        "# def forward(self, x):\n",
        "#   x= self.first_layer(x)\n",
        "#   x = F.relu(x)\n",
        "#   ...\n",
        "#   return x"
      ],
      "metadata": {
        "id": "ttpMl0A7DM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some constants that we will use for training\n",
        "INPUT_DIM = env.observation_space.shape[0]\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = env.action_space.n\n",
        "\n",
        "print('Input dimension:', INPUT_DIM)\n",
        "print('Hidden Dimension:', HIDDEN_DIM)\n",
        "print('Output dimension:', OUTPUT_DIM)\n",
        "\n",
        "\n",
        "# Instantiate the policy\n",
        "\n",
        "policy = Policy(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "# Optimizer, ADAM is a ameliorated version of SGD\n",
        "\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "# Discount_factor\n",
        "\n",
        "gamma = 0.95\n",
        "\n"
      ],
      "metadata": {
        "id": "tgcqYRleGvrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**\n",
        "\n",
        "In this part we perform the training of our policy, we will need some utilities.\n",
        "\n",
        "\n",
        "**Question 2.**  Write a function that takes the list of rewards of a trajectory and computes the sums $\\sum_{t' =t}^{T-1}\\gamma^{t'-t}r_{t'}$ for all $t$\n",
        "\n"
      ],
      "metadata": {
        "id": "yf0eM3C_Hv1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cumulative_returns(rewards):\n",
        "  returns = []\n",
        "  R = 0\n",
        "  T = len(rewards)\n",
        "\n",
        "  #TODO: Compute the returns list\n",
        "\n",
        "  # Hint, you can use a for loop from T-1 to 0 instead of 0 to T-1\n",
        "\n",
        "  # Transform it to tensor\n",
        "\n",
        "  returns = torch.tensor(returns, device=device)\n",
        "\n",
        "  return returns\n",
        "\n",
        "\n",
        "# Hint: we expect that returns[t] = rewards[t] + gamma * rewards[t+1] + ..."
      ],
      "metadata": {
        "id": "5IPQm-psIbfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3.** In order to interact for the environment, we need a function that calls the policy in a state, and return the action taken by the policy. To gain some time we also want this function to give us the associated log_probability.\n",
        "\n",
        "Code: select_action such that select_action(state) returns $a_t = π_θ(s_t)$ and  $\\log (\\pi_θ(a_t | s_t))$\n"
      ],
      "metadata": {
        "id": "zXr8UIOPJFM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: to sample random action with respect to the policy probabilities,\n",
        "# we use torch.distributions instead of numpy.random, because ot will enable us to compute gradients !\n",
        "\n",
        "def select_action(state):\n",
        "  state = torch.from_numpy(state).float().unsqueeze(0)  # Little trick to transform the state into a torch Tensor\n",
        "\n",
        "  action_prob = ... #TODO: Call the policy\n",
        "\n",
        "  dist = distributions.Categorical(action_prob)  # A categorical distribution is a distribution on a finite set\n",
        "\n",
        "  action = dist.sample() # Sample an action\n",
        "  log_prob = dist.log_prob(action) # Compute the log probability of the sampled action\n",
        "\n",
        "  action = action.item() # Transform the action from a Tensor to just its value (i.e. 0 or 1)\n",
        "\n",
        "  return action, log_prob\n"
      ],
      "metadata": {
        "id": "Y-RI-A2nYhxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4.** Now we can implement REINFORCE,  let's write a function that in a gym environment, and a value for $\\gamma$ and performs a REINFORCE step:\n",
        "\n",
        "- Generate a trajectory (also called episode in this context) by interacting with the environment\n",
        "\n",
        "- Compute the REINFORCE formula on this trajectory\n",
        "\n",
        "- Perform an optimization step (Adam)\n",
        "\n",
        "\n",
        "Recall we use the formula ($N=1$):\n",
        "\n",
        "$$ \\nabla_θ J(θ) ≈ \\sum_{t=0}^{T-1} \\gamma^t\\nabla_θ \\log (\\pi_θ(a_t | s_t)) \\sum_{t' =t}^{T-1}\\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "In the usual deep learning framework, we have a loss $\\mathcal{L}(θ)$ and we use the \"backward\" method to automatically compute $∇\\mathcal{L}(θ)$ and oerform an optimization step.\n",
        "\n",
        "Since we're aiming at maximizing $J$, our loss should be:\n",
        "\n",
        "$$\\mathcal{L}(θ) = - \\sum_{t=0}^{T-1} \\gamma^t \\log (\\pi_θ(a_t | s_t)) \\sum_{t' =t}^{T-1}\\gamma^{t'-t}r_{t'}  $$\n"
      ],
      "metadata": {
        "id": "ySG1qmsZbWf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce_step():\n",
        "  rewards = []\n",
        "  log_probs = []\n",
        "\n",
        "  policy.train()  # Turns on \"training mode\" for the network\n",
        "\n",
        "  state = env.reset()\n",
        "\n",
        "  for t in range(1000):   # We would want to put while True but is a bad idea to it inside the training of a NN\n",
        "    action, log_prob = ... # TODO: Follow the policy\n",
        "\n",
        "    next_state, reward, terminated, truncated = env.step(action)\n",
        "    log_probs.append(log_prob)\n",
        "    rewards.append(reward)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    if terminated or truncated:  # It means the episode is over (we reached T)\n",
        "      break\n",
        "\n",
        "  returns = ... # Compute returns\n",
        "\n",
        "  # Transform log_probs into a torch object\n",
        "  log_probs = torch.cat(log_probs).to(device)\n",
        "\n",
        "  # powers of gamma\n",
        "  gamma_t = ... #TODO: compute an array which contains [1, gamma, gamma**2,...]\n",
        "\n",
        "\n",
        "  gamma_t = torch.tensor(gamma_t, device=device)\n",
        "\n",
        "\n",
        "  loss = ... # Compute the loss, you can use .sum() to compute the sum of a tensor\n",
        "\n",
        "  # Perform the gradient step - torch magic\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # return the value of the loss and trajectory reward so we can make plots\n",
        "\n",
        "  return loss.item(), sum(rewards)\n"
      ],
      "metadata": {
        "id": "s7kvIJwNchpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.** Now we can optimize our policy for a given number of reinforce steps, you can play with the parameters to see if you get better results !"
      ],
      "metadata": {
        "id": "fC-wKr7hl4-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "policy = Policy(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)  # You can also play with the learning rate\n",
        "gamma = 0.99\n",
        "\n",
        "\n",
        "num_reinforce_steps = 1000 # If you put 1000 it should reach max reward, but if you don't want to take too much time you can put 500, and we'll reach max reward in next questions\n",
        "losses = []\n",
        "rewards = []\n",
        "\n",
        "for step in range(num_reinforce_steps):\n",
        "  loss, reward = ... # TODO\n",
        "\n",
        "  if step % 50 == 0:\n",
        "    print(f'| Episode: {step:3} | Reward: {reward:5.1f} | ')\n",
        "\n",
        "  losses.append(loss)\n",
        "  rewards.append(reward)"
      ],
      "metadata": {
        "id": "gPfrTQNSncnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,2)\n",
        "\n",
        "axs[0].plot(losses)\n",
        "axs[0].set_title(\"Evolution of the policy loss during training\")\n",
        "axs[0].set_xlabel(\"episode\")\n",
        "axs[0].set_ylabel(\" policy loss\")\n",
        "\n",
        "\n",
        "axs[1].plot(rewards)\n",
        "axs[1].set_title(\"Evolution of the rewards during training\")\n",
        "axs[1].set_xlabel(\"episode\")\n",
        "axs[1].set_ylabel(\"reward\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxztu5lnpU3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see training is quite **unstable**, to make the curves more readable, we can plot the average reward on the 25 past episodes."
      ],
      "metadata": {
        "id": "AtHIHKyNw5NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_on_last_episodes(rewards, n_episodes=25):\n",
        "  mean_rewards = [rewards[0]]\n",
        "  #TODO:  mean_rewards[i] should contain the average reward between i and i - n_episode\n",
        "  # Pay attention if i is smaller than n_episode just take the i last episodes\n",
        "\n",
        "\n",
        "  return mean_rewards"
      ],
      "metadata": {
        "id": "u078oaxCp6hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rewards = compute_average_on_last_episodes(rewards, 25)\n",
        "\n",
        "plt.plot(mean_rewards)\n",
        "plt.title(\"Evolution of the mean reward during training\")\n",
        "plt.xlabel(\"episode\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vWjc7rwmyFMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment**:"
      ],
      "metadata": {
        "id": "IcGl9WN5A_8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On Cartpole the perfect score is 500, we see that our policy was able to reach this score, but because of instability it might fall out of the optima, a strategy is to stop the training when the policy performs good enough (>475) on 25 consecutive episodes.\n",
        "\n",
        "\n",
        "Also, we are looking at the training reward, but in training our policy is perturbed by the dropout, and the fact it samples its action randomly, what is usually done, is to perform deterministic evals of the policy.\n",
        "\n",
        "\n",
        "**Question 6.** Write a code that evaluate the deterministic policy, i.e. that plays the action: $a_t = \\mathrm{argmax}(\\pi_\\theta (s_t))$ instead of sampling."
      ],
      "metadata": {
        "id": "7mlGklWn8bYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy():\n",
        "  policy.eval()  # put the network in eval mode disables the dropout !\n",
        "  state = env.reset()\n",
        "  rewards = []\n",
        "\n",
        "  for i in range(1000):\n",
        "    state = torch.tensor(state).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradients computation\n",
        "      action_prob = ... # Get the policy's output\n",
        "\n",
        "    action = ... # Select the most probable action, you might need to use .item() to get a value\n",
        "\n",
        "    next_state, reward, terminated, truncated = ... # play the action\n",
        "    state= next_state\n",
        "\n",
        "    rewards.append(reward)\n",
        "\n",
        "    if terminated or truncated: break\n",
        "\n",
        "\n",
        "  return sum(rewards)\n"
      ],
      "metadata": {
        "id": "XHN9Aolqze2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Our current policy got a reward of {evaluate_policy()} during one evaluation episode\")"
      ],
      "metadata": {
        "id": "ZdoOKrkZRyMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train in a proper way:\n",
        "\n",
        "- Perfom training steps\n",
        "\n",
        "- Evaluate the policy frequently\n",
        "\n",
        "- Stop the training when our policy reaches the threshold"
      ],
      "metadata": {
        "id": "meoFYeqGTfpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "policy = Policy(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)  # You can also play with the learning rate\n",
        "gamma = 0.99\n",
        "\n",
        "\n",
        "reward_treshold = 475\n",
        "num_episodes_above_threshold = 25\n",
        "\n",
        "num_reinforce_steps = 1000\n",
        "losses = []\n",
        "train_rewards = []\n",
        "test_rewards = []\n",
        "\n",
        "for step in range(num_reinforce_steps):\n",
        "  loss, train_reward = reinforce_step()\n",
        "  test_reward = evaluate_policy()\n",
        "\n",
        "  losses.append(loss)\n",
        "  train_rewards.append(train_reward)\n",
        "  test_rewards.append(test_reward)\n",
        "\n",
        "  mean_train_rewards = compute_average_on_last_episodes(train_rewards, n_episodes=num_episodes_above_threshold)\n",
        "  mean_test_rewards = compute_average_on_last_episodes(test_rewards, n_episodes=num_episodes_above_threshold)\n",
        "\n",
        "  if mean_test_rewards[-1] > reward_treshold:\n",
        "    print(f\"Reached convegence in {step + 1} episodes\")\n",
        "    break\n",
        "\n",
        "  if step % 50 == 0:\n",
        "    print(f'| Episode: {step:3} | Mean train reward: {mean_train_rewards[-1]:5.1f} | Mean test reward: {mean_test_rewards[-1]:5.1f} | ')\n",
        "\n"
      ],
      "metadata": {
        "id": "l2E5sK0CRzrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_rewards, label='Test_reward')\n",
        "plt.axhline(reward_treshold, color='r', label = 'performance threshold')\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6pY08PkET4aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now the best part let's visualize our policy :"
      ],
      "metadata": {
        "id": "NJIpQQvVbU4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Video utils (ignore them)"
      ],
      "metadata": {
        "id": "5Ikax4kDkSHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some installations to make it work on colab, I wouldn't recommend to run this on a local jupyter notebook\n",
        "\n",
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "9VqJosWkc5iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "import os"
      ],
      "metadata": {
        "id": "m-kjgZxeeXFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "3uQj5tM1gtv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env, video_name):\n",
        "  env = RecordVideo(env, './video')\n",
        "  return env"
      ],
      "metadata": {
        "id": "W7NRnAAGkBPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Visualization"
      ],
      "metadata": {
        "id": "wC4FQdLUkXxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Policy\n",
        "\n",
        "# Delete the previous video\n",
        "!rm -rf video\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state = env.reset()\n",
        "env = wrap_env(env, 'random_policy')\n",
        "\n",
        "while True:\n",
        "  env.render()\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  observation, reward, terminated, truncated = env.step(action)\n",
        "\n",
        "\n",
        "  if terminated or truncated:\n",
        "    break\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "YvF5v2HAkJEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our Policy\n",
        "\n",
        "# Delete the previous video\n",
        "!rm -rf video\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state = env.reset()\n",
        "env = wrap_env(env, 'random_policy')\n",
        "\n",
        "policy.eval()\n",
        "\n",
        "while True:\n",
        "  env.render()\n",
        "\n",
        "  state = torch.tensor(state).float().unsqueeze(0)\n",
        "\n",
        "  #  TODO Play the most probable action as if we're in eval mode\n",
        "  # Don't forget to put a .item() to change the action to it's value\n",
        "  action_prob = ...\n",
        "  action = ...\n",
        "\n",
        "  next_state, reward, terminated, truncated = ...\n",
        "\n",
        "  state = next_state\n",
        "\n",
        "\n",
        "  if terminated or truncated:\n",
        "    break\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "0GhF1bxWkxfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jkhBki-1DG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}